import nltk.classify.util
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import movie_reviews
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import os
import random
#
#
rootdir = "C:\\Users\\Dell\\Desktop\\Enron Spam"
#
#
print(os.path.split("C:\\Users\\Dell\\Desktop\\Enron Spam\\enron1\\ham"))
print(os.path.split("C:\\Users\\Dell\\Desktop\\Enron Spam\\enron1\\ham")[0])
print(os.path.split("C:\\Users\\Dell\\Desktop\\Enron Spam\\enron1\\ham")[1])
#
#
for directories, subdirs, files in os.walk(rootdir):
    if (os.path.split(directories)[1]  == 'ham'):
        print(directories, subdirs, len(files))
    
    if (os.path.split(directories)[1]  == 'spam'):
        print(directories, subdirs, len(files))
#
#
ham_list = []
spam_list = []

for directories, subdirs, files in os.walk(rootdir):
    if (os.path.split(directories)[1]  == 'ham'):
        for filename in files:      
            with open(os.path.join(directories, filename), encoding="latin-1") as f:
                data = f.read()
                ham_list.append(data)
    
    if (os.path.split(directories)[1]  == 'spam'):
        for filename in files:
            with open(os.path.join(directories, filename), encoding="latin-1") as f:
                data = f.read()
                spam_list.append(data)


print(ham_list[0])
print(spam_list[0])
#
#

def create_word_features(words):
    my_dict = dict( [ (word, True) for word in words] )
    return my_dict

create_word_features(["the", "quick", "brown", "quick", "a", "fox"])
#
#
import nltk
nltk.download('punkt')
ham_list = []
spam_list = []


for directories, subdirs, files in os.walk(rootdir):
    if (os.path.split(directories)[1]  == 'ham'):
        for filename in files:      
            with open(os.path.join(directories, filename), encoding="latin-1") as f:
                data = f.read()
                
                # The data we read is one big string. We need to break it into words.
                words = word_tokenize(data)
                
                ham_list.append((create_word_features(words), "ham"))
    
    if (os.path.split(directories)[1]  == 'spam'):
        for filename in files:
            with open(os.path.join(directories, filename), encoding="latin-1") as f:
                data = f.read()
                words = word_tokenize(data)
                
                spam_list.append((create_word_features(words), "spam"))
print(ham_list[0])
print(spam_list[0])
#
#
combined_list = ham_list + spam_list
print(len(combined_list))

random.shuffle(combined_list)
#
#

training_part = int(len(combined_list) * .7)

print(len(combined_list))

training_set = combined_list[:training_part]

test_set =  combined_list[training_part:]

print (len(training_set))
print (len(test_set))
#
#

classifier = NaiveBayesClassifier.train(training_set)


accuracy = nltk.classify.util.accuracy(classifier, test_set)

print("Accuracy is: ", accuracy * 100)
##
##
classifier.show_most_informative_features(20)
##
##

msg1 = 'THe cows were killed at a large scale with utmst brutality and transported..this needs to stop..we want violence and fight against community..'
msg2 = 'To start off, I have a 6 new videos + transcripts in the members section. In it, we analyse the Enron email dataset, half a million files, spread over 2.5GB. Its about 1.5 hours of  video.I have also created a Conda environment for running the code (both free and member lessons). This is to ensure everyone is running the same version of libraries, preventing the Works on my machine problems. If you get a second, do you mind trying it here?'

##
##

words = word_tokenize(msg1)
features = create_word_features(words)
print("Message 1 is :" ,classifier.classify(features))

************************************************************************************************************************************************************************************************************************************************************************************************************************************************
